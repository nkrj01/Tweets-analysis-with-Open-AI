{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vLDkQp78xQ_dDl1O-UapFN_cp5jIzwo4",
      "authorship_tag": "ABX9TyNaUVj0zM5Ak3r4jrr5iFnC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkrj01/Tweets-analysis-with-Open-AI/blob/main/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating Ada and tf-idf embedding**\n",
        "This notebook is for creating ada embedding using openAI and tf-idf embedding using sk-learn. Embedding are saved as a csv file and use in a separate notebook for classification."
      ],
      "metadata": {
        "id": "sTfIwam6qq7s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfRqjYculBT6"
      },
      "outputs": [],
      "source": [
        "! pip install cohere\n",
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "B70Jc8AYo7FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Helper functions**"
      ],
      "metadata": {
        "id": "emO24Bf_rshB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.externals._packaging.version import collections\n",
        "def removeHyperlinks(col):\n",
        "  # Regular expression to match URLs (both http/https and www variants)\n",
        "  url_pattern = r'https?://\\S+|www\\.\\S+'\n",
        "\n",
        "  # Use the sub() function to replace all URLs with an empty string\n",
        "  text_without_links = re.sub(url_pattern, '', col)\n",
        "  return text_without_links\n",
        "\n",
        "def removeSpecialCharacters(col):\n",
        "    # Define a regular expression pattern to match special characters at the start and end of the sentence\n",
        "    pattern = r'[^A-Za-z\\s]+'\n",
        "\n",
        "    # Use the sub() function to remove special characters\n",
        "    cleaned_sentence = re.sub(pattern, '', col)\n",
        "    cleaned_sentence = cleaned_sentence.rstrip()\n",
        "    cleaned_sentence = re.sub(r'\\s+', ' ', cleaned_sentence)\n",
        "\n",
        "    return cleaned_sentence\n",
        "\n",
        "\n",
        "def similarity_ratio(a, b):\n",
        "  # this function matches two strings and returns the ratio of similarity between two string\n",
        "  # an output of 1 mean exact same\n",
        "  return SequenceMatcher(a, b).ratio()\n",
        "\n",
        "def removeDuplicates(df, threshold=0.95):\n",
        "  rows_to_drop = []\n",
        "  for i, row1 in df.iterrows():\n",
        "    if i% 10 == 0 and i!=0:\n",
        "      print(i)\n",
        "    for j, row2 in df.iterrows():\n",
        "\n",
        "      if j>i and i not in rows_to_drop and j not in rows_to_drop:\n",
        "        similarity = similarity_ratio(row1[\"text\"], row2[\"text\"])\n",
        "        if similarity>threshold:\n",
        "          rows_to_drop.append(j)\n",
        "  df_cleaned = df.drop(rows_to_drop)\n",
        "  return df_cleaned\n",
        "\n",
        "\n",
        "def getAdaEmbedding(train_text: list, model=\"text-embedding-ada-002\") -> list:\n",
        "  total_size = len(train_text)\n",
        "  batch_end = 0\n",
        "  batch_size = 500\n",
        "  n_steps = int(total_size/batch_size) + 1\n",
        "  ada_embedding = []\n",
        "  for i in range(n_steps):\n",
        "    batch_start = batch_end\n",
        "    batch_end = batch_start+batch_size\n",
        "    if batch_end<=total_size:\n",
        "      pass\n",
        "    else:\n",
        "      batch_end = total_size\n",
        "      batch_size = total_size % batch_size\n",
        "    text = train_text[batch_start:batch_end]\n",
        "    output = openai.Embedding.create(input = text, model=model)\n",
        "    for j in range(batch_size):\n",
        "      ada_embedding.append(output['data'][j]['embedding'])\n",
        "  return ada_embedding"
      ],
      "metadata": {
        "id": "UbFB5Gewr0HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Import**"
      ],
      "metadata": {
        "id": "Ykp_gjXZ3ME8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/OpenAI/train.csv\")\n",
        "df_test = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/OpenAI/test.csv\")\n",
        "df_train.shape"
      ],
      "metadata": {
        "id": "TnguVXdp2boL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cleaning and storing the cleaned text**"
      ],
      "metadata": {
        "id": "5ZhwhBYxOcXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text cleaning function\n",
        "def clean_text(df_train):\n",
        "  df_train[\"text\"] = df_train[\"text\"].astype(\"string\")\n",
        "  df_train[\"text\"] = df_train[\"text\"].apply(removeHyperlinks)\n",
        "  df_train[\"text\"] = df_train[\"text\"].apply(removeSpecialCharacters)\n",
        "  df_train = removeDuplicates(df_train)\n",
        "  df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\" \")\n",
        "  df_train[\"text\"] = df_train[\"text\"] + \". \" + df_train[\"keyword\"] # join keyword and text column\n",
        "  return df_train\n",
        "\n",
        "df_train_clean = clean_text(df_train)\n",
        "df_test_clean = clean_text(df_test)\n",
        "df_train.to_csv(r'/content/drive/MyDrive/Colab Notebooks/OpenAI/train_clean.csv')\n",
        "df_test.to_csv(r'/content/drive/MyDrive/Colab Notebooks/OpenAI/test_clean.csv')"
      ],
      "metadata": {
        "id": "FbP92kBrKIaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ada Embedding using Open AI**"
      ],
      "metadata": {
        "id": "Z7dmZ59I1U3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('openai')\n",
        "\n",
        "# importing the cleaned text\n",
        "df_train = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/OpenAI/train_clean.csv')\n",
        "train_text = df_train[\"text\"].to_list()\n",
        "\n",
        "# Ada embedding\n",
        "ada_embedding = getAdaEmbedding(train_text)\n",
        "df_train[\"ada_embedding\"] = ada_embedding\n",
        "\n",
        "# saving embedded vectors\n",
        "df_train.to_csv(r'/content/drive/MyDrive/Colab Notebooks/OpenAI/train_ada_embedded.csv', index=False)"
      ],
      "metadata": {
        "id": "yC33p4-d4l2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TfIdf encoder**"
      ],
      "metadata": {
        "id": "rH76pbEO41KS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the cleaned text\n",
        "df_train = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/OpenAI/train_clean.csv')\n",
        "df_train[\"keyword\"] = df_train[\"keyword\"].fillna(\" \")\n",
        "df_train[\"text\"] = df_train[\"text\"] + \". \" + df_train[\"keyword\"]\n",
        "\n",
        "# function for removing the stop words and lemmatizing, i.e., pre processing.\n",
        "def pre_processing(text):\n",
        "  # Initialize the lemmatizer and stop words\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  punctuation = set(string.punctuation)\n",
        "\n",
        "  processed_text = []\n",
        "  for sentence in text:\n",
        "    tokens = word_tokenize(sentence)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in punctuation]\n",
        "    processed_text.append(' '.join(lemmatized_tokens))\n",
        "  return processed_text\n",
        "\n",
        "# text pre-processing\n",
        "text = df_train[\"text\"].to_list()\n",
        "processed_text = pre_processing(text)\n",
        "\n",
        "# tf-idf encoding\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_text)\n",
        "tfidf = tfidf_matrix.toarray().tolist()\n",
        "df_train[\"tfidf\"] = tfidf\n",
        "\n",
        "# saving the encoded vectors\n",
        "df_train.to_csv(r'/content/drive/MyDrive/Colab Notebooks/OpenAI/train_tfidf_encoded.csv', index=False)"
      ],
      "metadata": {
        "id": "OuL-5U35EpSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}